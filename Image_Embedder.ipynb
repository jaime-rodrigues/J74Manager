{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaime-rodrigues/J74Manager/blob/master/Image_Embedder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy6599k4Z1WB"
      },
      "outputs": [],
      "source": [
        "# Atualizar pacotes e adicionar repositório oficial do PostgreSQL\n",
        "!apt-get update\n",
        "!apt-get install -y wget gnupg2\n",
        "!wget -qO - https://www.postgresql.org/media/keys/ACCC4CF8.asc | apt-key add -\n",
        "!echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" | tee /etc/apt/sources.list.d/pgdg.list\n",
        "!apt-get update\n",
        "\n",
        "# Instalar PostgreSQL 17 e a extensão pgvector\n",
        "!apt-get update\n",
        "!apt-get install -y postgresql-17 postgresql-17-pgvector postgresql-contrib postgresql-server-dev-17\n",
        "!service postgresql start\n",
        "\n",
        "# Criar um usuário e um banco de dados no PostgreSQL\n",
        "!sudo -u postgres psql -c \"CREATE USER colab WITH SUPERUSER PASSWORD 'colab';\"\n",
        "!sudo -u postgres psql -c \"CREATE DATABASE colab OWNER colab;\"\n",
        "\n",
        "print(\"PostgreSQL instalado e configurado!\")\n",
        "# Instalar a extensão pgvector\n",
        "!sudo -u postgres psql -d colab -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n",
        "print(\"PostgreSQL com pgvector instalado e configurado!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKzBxKGvq4Op",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install retry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faWRvFtA79Hn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import psycopg2\n",
        "from psycopg2.extras import execute_values\n",
        "from psycopg2 import OperationalError, InterfaceError\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple\n",
        "from retry import retry\n",
        "from pathlib import Path\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import threading\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "source": [
        "# import zipfile\n",
        "\n",
        "# # 1. Specify the path to the zip file and the destination folder\n",
        "# zip_file_path = '/content/drive/MyDrive/DevMaster/Fotos/produtos/Licuri/Pronto.zip'\n",
        "# destination_folder = '/content/drive/MyDrive/DevMaster/Fotos/produtos/Licuri/Pronto'  # Create a folder to extract to\n",
        "\n",
        "# # 2. Create the destination folder if it doesn't exist\n",
        "# os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "# # 3. Open the zip file and extract its contents\n",
        "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(destination_folder)\n",
        "\n",
        "# print(f\"Files extracted to: {destination_folder}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "D6aa6GccAkxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uxnZP0c8AOk"
      },
      "outputs": [],
      "source": [
        "# Configurações globais\n",
        "DATABASE_CONFIG = {\n",
        "    \"host\": \"localhost\",\n",
        "    \"database\": \"colab\",\n",
        "    \"user\": \"colab\",\n",
        "    \"password\": \"colab\",\n",
        "    \"port\": \"5432\"\n",
        "}\n",
        "\n",
        "IMAGE_EXTS = ('.jpg', '.jpeg', '.png', '.webp')\n",
        "BATCH_SIZE = 32  # Tamanho do lote para inserção no banco de dados\n",
        "EMBEDDING_DIM = 768  # ResNet50 output size\n",
        "CLIP_BASE = \"openai/clip-vit-large-patch14\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7taPigS8aJo"
      },
      "outputs": [],
      "source": [
        "# 2. Gerenciamento de banco de dados aprimorado\n",
        "class DatabaseManager:\n",
        "    def __init__(self, config: dict, backup_file_path='/content/backup.sql'):\n",
        "        self.config = config\n",
        "        self.backup_file_path = backup_file_path\n",
        "        self._connect_and_restore()\n",
        "\n",
        "    @retry((OperationalError, InterfaceError), tries=3, delay=2)\n",
        "    def _connect_with_retry(self):\n",
        "        \"\"\"Conexão com retentativa automática\"\"\"\n",
        "        return psycopg2.connect(**self.config)\n",
        "\n",
        "    def _connect_and_restore(self):\n",
        "        \"\"\"Conecta ao banco de dados e restaura se o backup existir.\"\"\"\n",
        "        try:\n",
        "            # Tenta conectar ao banco de dados\n",
        "            self.conn = self._connect_with_retry()\n",
        "\n",
        "            # Verifica se o arquivo de backup existe\n",
        "            if os.path.exists(self.backup_file_path):\n",
        "                # Restaura o banco de dados a partir do backup\n",
        "                self._restore_database()\n",
        "                print(f\"Banco de dados restaurado de: {self.backup_file_path}\")\n",
        "            else:\n",
        "              self.recreate_table()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao conectar ou restaurar o banco de dados: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _restore_database(self):\n",
        "        \"\"\"Restaura o banco de dados a partir do arquivo de backup.\"\"\"\n",
        "        try:\n",
        "            # Obtem as configurações do banco de dados\n",
        "            host = self.config['host']\n",
        "            port = self.config['port']\n",
        "            database = self.config['database']\n",
        "            user = self.config['user']\n",
        "            password = self.config['password']\n",
        "\n",
        "            # Comando psql para restauração\n",
        "            command = [\n",
        "                'psql',\n",
        "                '-h', host,\n",
        "                '-p', port,\n",
        "                '-U', user,\n",
        "                '-d', database,\n",
        "                '-f', self.backup_file_path\n",
        "            ]\n",
        "\n",
        "            # Executa o comando psql\n",
        "            process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            process.communicate(input=password.encode())\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao restaurar o banco de dados: {e}\")\n",
        "            raise\n",
        "\n",
        "    def recreate_table(self):\n",
        "        \"\"\"Drop and recreate the table with the unique constraint.\"\"\"\n",
        "        with self.conn.cursor() as cursor:\n",
        "            cursor.execute(\"DROP TABLE IF EXISTS image_embeddings;\")\n",
        "            self.conn.commit()\n",
        "        self.create_table()\n",
        "\n",
        "    def create_table(self):\n",
        "        \"\"\"Cria tabela com índice especializado\"\"\"\n",
        "        with self.conn.cursor() as cursor:\n",
        "            cursor.execute(f\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS image_embeddings (\n",
        "                    id SERIAL PRIMARY KEY,\n",
        "                    filename VARCHAR(255) NOT NULL,\n",
        "                    filepath VARCHAR(4096) NOT NULL,\n",
        "                    embedding vector({EMBEDDING_DIM}) NOT NULL\n",
        "                );\n",
        "            \"\"\")\n",
        "            self.conn.commit()\n",
        "\n",
        "    def create_table_ivfflat(self):\n",
        "        \"\"\"Cria tabela com índice especializado\"\"\"\n",
        "        with self.conn.cursor() as cursor:\n",
        "            cursor.execute(f\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS image_embeddings (\n",
        "                    id SERIAL PRIMARY KEY,\n",
        "                    filename VARCHAR(255) NOT NULL,\n",
        "                    filepath VARCHAR(4096) NOT NULL,\n",
        "                    embedding vector({EMBEDDING_DIM}) NOT NULL\n",
        "                );\n",
        "\n",
        "                CREATE INDEX IF NOT EXISTS embedding_idx\n",
        "                ON image_embeddings USING ivfflat (embedding vector_l2_ops)\n",
        "                WITH (lists = 100);\n",
        "            \"\"\")\n",
        "            self.conn.commit()\n",
        "\n",
        "    def create_table_hnsw(self):\n",
        "        \"\"\"Cria tabela com índice especializado\"\"\"\n",
        "        with self.conn.cursor() as cursor:\n",
        "            cursor.execute(f\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS image_embeddings (\n",
        "                    id SERIAL PRIMARY KEY,\n",
        "                    filename VARCHAR(255) NOT NULL,\n",
        "                    filepath VARCHAR(4096) NOT NULL,\n",
        "                    embedding vector({EMBEDDING_DIM}) NOT NULL\n",
        "                );\n",
        "\n",
        "                CREATE INDEX IF NOT EXISTS embedding_idx\n",
        "                ON image_embeddings USING hnsw (embedding vector_l2_ops)\n",
        "                WITH (m = 16, ef_construction = 100);\n",
        "            \"\"\")\n",
        "            self.conn.commit()\n",
        "\n",
        "    @retry((OperationalError, InterfaceError), tries=3, delay=1)\n",
        "    def insert_embeddings_batch(self, records: List[Tuple[str, str, list]]):\n",
        "        \"\"\"Inserção em lote otimizada\"\"\"\n",
        "        query = \"\"\"\n",
        "            INSERT INTO image_embeddings (filename, filepath, embedding)\n",
        "            VALUES %s;\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with self.conn.cursor() as cursor:\n",
        "                execute_values(cursor, query, records, page_size=BATCH_SIZE)\n",
        "                self.conn.commit()\n",
        "        except Exception as e:\n",
        "            self.conn.rollback()\n",
        "            raise\n",
        "\n",
        "    @retry((OperationalError, InterfaceError), tries=3, delay=1)\n",
        "    def search_similar(self, embedding: np.ndarray, top_k: int = 5) -> List[Tuple]:\n",
        "        \"\"\"Busca com tratamento de erros\"\"\"\n",
        "        embedding_str = ','.join(map(str, embedding.tolist()))  # Formata o embedding\n",
        "        with self.conn.cursor() as cursor:\n",
        "            cursor.execute(f\"\"\"\n",
        "                SELECT filename, filepath, 1 - (embedding <-> ARRAY[{embedding_str}]::vector) as similarity\n",
        "                FROM image_embeddings\n",
        "                ORDER BY embedding <-> ARRAY[{embedding_str}]::vector\n",
        "                LIMIT {top_k};\n",
        "            \"\"\")\n",
        "            return cursor.fetchall()\n",
        "\n",
        "    def listar_primeiros_100_registros(self):\n",
        "        \"\"\"\n",
        "        Lista os primeiros 100 registros da tabela image_embeddings.\n",
        "\n",
        "        Args:\n",
        "            None\n",
        "\n",
        "        Returns:\n",
        "            Uma lista de tuplas, onde cada tupla representa uma linha da tabela.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with self.conn.cursor() as cursor:\n",
        "                cursor.execute(\"SELECT * FROM image_embeddings LIMIT 100;\")\n",
        "                registros = cursor.fetchall()\n",
        "            return registros\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao buscar registros: {e}\")\n",
        "            return []\n",
        "\n",
        "    def close(self):\n",
        "        if self.conn:\n",
        "            self.conn.close()\n",
        "\n",
        "    def backup_database(self):\n",
        "        \"\"\"\n",
        "        Faz o backup do banco de dados.\n",
        "\n",
        "        Args:\n",
        "            backup_file_path: Caminho para o arquivo de backup.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Obtem as configurações do banco de dados\n",
        "            host = self.config['host']\n",
        "            port = self.config['port']\n",
        "            database = self.config['database']\n",
        "            user = self.config['user']\n",
        "            password = self.config['password']\n",
        "\n",
        "            # Comando pg_dump\n",
        "            command = [\n",
        "                'pg_dump',\n",
        "                '-h', host,\n",
        "                '-p', port,\n",
        "                '-U', user,\n",
        "                '-d', database,\n",
        "                '-f', self.backup_file_path\n",
        "            ]\n",
        "\n",
        "            # Executa o comando pg_dump\n",
        "            process = subprocess.Popen(command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            process.communicate(input=password.encode())\n",
        "\n",
        "            print(f\"Backup do banco de dados criado em: {self.backup_file_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao criar o backup do banco de dados: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjkZl4FP8WO8"
      },
      "outputs": [],
      "source": [
        "# 1. Modelo CLIP para embeddings semânticos\n",
        "class CLIPEmbedder:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model = CLIPModel.from_pretrained(CLIP_BASE)\n",
        "        self.processor = CLIPProcessor.from_pretrained(CLIP_BASE)\n",
        "        self.model.to(self.device).eval()\n",
        "\n",
        "    @retry(tries=3, delay=1)\n",
        "    def generate_embedding(self, image: Image) -> np.ndarray:\n",
        "        \"\"\"Gera embedding usando CLIP com tratamento de erros\"\"\"\n",
        "        try:\n",
        "            inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
        "            with torch.no_grad():\n",
        "                features = self.model.get_image_features(**inputs)\n",
        "            return features.cpu().numpy().squeeze().astype(np.float32)\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar imagem: {str(e)}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTn34bv08g7Q"
      },
      "outputs": [],
      "source": [
        "# 3. Interface gráfica simplificada\n",
        "class ImageSearchGUI:\n",
        "    def __init__(self, db_manager: DatabaseManager, embedder: CLIPEmbedder):\n",
        "        self.db = db_manager\n",
        "        self.embedder = embedder\n",
        "\n",
        "    def process_folder(self, folder):\n",
        "        \"\"\"Seleção de pasta via diálogo\"\"\"\n",
        "        print(folder)\n",
        "        if folder:\n",
        "            self.process_images(Path(folder))\n",
        "            self.db.backup_database()\n",
        "\n",
        "    def process_folders(self, diretorio = \"/content/drive/MyDrive/DevMaster/Fotos/produtos/\", folders =['Anel','Chocker','Brinco','Colar', 'Conjuntos']):\n",
        "        for folder in folders:\n",
        "          self.process_folder(diretorio+folder)\n",
        "\n",
        "    def search_image(self, filepath):\n",
        "        \"\"\"Seleção de imagem para busca\"\"\"\n",
        "        if filepath:\n",
        "            print(filepath)\n",
        "            image = Image.open(filepath).convert(\"RGB\")\n",
        "            self.show_results(image)\n",
        "\n",
        "    def process_images(self, folder: Path):\n",
        "        \"\"\"Processamento em lote com barra de progresso, thread separada e subpastas\"\"\"\n",
        "        all_image_files = []\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(IMAGE_EXTS):\n",
        "                    all_image_files.append(os.path.join(root, file))\n",
        "\n",
        "        total_images = len(all_image_files)\n",
        "        progress_bar = tqdm(total=total_images, desc=\"Processando Imagens\")\n",
        "\n",
        "        def process_images_thread():\n",
        "            records = []\n",
        "            for i, image_file in enumerate(all_image_files):\n",
        "                try:\n",
        "                  # Chama a função de transformação para gerar novas imagens\n",
        "                  imagens_transformadas = self.transformar_imagem(image_file)\n",
        "\n",
        "                  # Gera embeddings para cada imagem transformada\n",
        "                  for imagem_transformada in imagens_transformadas:\n",
        "                      embedding = self.embedder.generate_embedding(imagem_transformada)\n",
        "                      records.append((os.path.basename(image_file), image_file, embedding.tolist()))  # Armazena o nome da imagem original\n",
        "\n",
        "                      if len(records) >= BATCH_SIZE:\n",
        "                          self.db.insert_embeddings_batch(records)\n",
        "                          records = []\n",
        "\n",
        "                  progress_bar.update(1)\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro ao processar {image_file}: {e}\")\n",
        "            # Inserir quaisquer registros restantes\n",
        "            if records:\n",
        "                self.db.insert_embeddings_batch(records)\n",
        "\n",
        "        thread = threading.Thread(target=process_images_thread)\n",
        "        thread.start()\n",
        "\n",
        "        # Aguarda o término da thread antes de retornar\n",
        "        thread.join()\n",
        "        progress_bar.close()\n",
        "\n",
        "    def show_results(self, image: Image):\n",
        "        \"\"\"Exibe resultados com visualização\"\"\"\n",
        "        try:\n",
        "            embedding = self.embedder.generate_embedding(image)\n",
        "            results = self.db.search_similar(embedding)\n",
        "\n",
        "            # Plot dos resultados\n",
        "            plt.figure(figsize=(15, 10))\n",
        "\n",
        "            # Imagem de consulta\n",
        "            plt.subplot(2, 3, 1)\n",
        "            plt.imshow(image)\n",
        "            plt.title(\"Query Image\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Imagens similares\n",
        "            for i, (filename, filepath, similarity) in enumerate(results, 2):\n",
        "                plt.subplot(2, 3, i)\n",
        "                plt.imshow(Image.open(filepath))\n",
        "                plt.title(f\"{filename}\\nSimilarity: {similarity:.2f}\")\n",
        "                plt.axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao processar imagem: {e}\")\n",
        "            pass\n",
        "\n",
        "    def listar_primeiros(self):\n",
        "        \"\"\"Seleção de pasta via diálogo\"\"\"\n",
        "        return self.db.listar_primeiros_100_registros()\n",
        "\n",
        "    def on_close(self):\n",
        "        self.db.close()\n",
        "        self.root.destroy()\n",
        "\n",
        "    def transformar_imagem(self, image_path):\n",
        "        \"\"\"Aplica transformações na imagem original e retorna uma lista de imagens transformadas.\"\"\"\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "        # Define as transformações a serem aplicadas\n",
        "        transformacoes = [\n",
        "            T.RandomRotation(degrees=30),\n",
        "            T.RandomResizedCrop(size=(224, 224)),\n",
        "            T.RandomHorizontalFlip(),\n",
        "            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "            # Adicione mais transformações conforme necessário\n",
        "        ]\n",
        "\n",
        "        # Aplica as transformações e gera novas imagens\n",
        "        imagens_transformadas = []\n",
        "        for transformacao in transformacoes:\n",
        "            transformed_image = transformacao(image)\n",
        "            imagens_transformadas.append(transformed_image)\n",
        "\n",
        "        # Adiciona a imagem original à lista\n",
        "        imagens_transformadas.append(image)\n",
        "\n",
        "        return imagens_transformadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMAtI7bU8pJY"
      },
      "outputs": [],
      "source": [
        "# 4. Funções principais otimizadas\n",
        "def main():\n",
        "    # Inicialização de componentes\n",
        "    embedder = CLIPEmbedder()\n",
        "    db = DatabaseManager(DATABASE_CONFIG, backup_file_path='/content/drive/MyDrive/colab_backup.sql')\n",
        "\n",
        "    # Interface gráfica\n",
        "    gui = ImageSearchGUI(db, embedder)\n",
        "\n",
        "    return gui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ4NCJ70z-ub"
      },
      "outputs": [],
      "source": [
        "gui = main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gui.listar_primeiros()"
      ],
      "metadata": {
        "id": "h7iP_OGiLwMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn6TiENCCx6Q"
      },
      "outputs": [],
      "source": [
        "gui.search_image(\"/content/drive/MyDrive/DevMaster/Fotos/modificada/103_1.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vK4ygw40fV8"
      },
      "outputs": [],
      "source": [
        "gui.search_image(\"/content/drive/MyDrive/DevMaster/Fotos/modificada/103_2.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11RjaqTcNo8U"
      },
      "outputs": [],
      "source": [
        "gui.search_image(\"/content/drive/MyDrive/DevMaster/Fotos/modificada/IMG_7224_modificada.jpeg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpcSsGImNpZc"
      },
      "outputs": [],
      "source": [
        "gui.search_image(\"/content/drive/MyDrive/DevMaster/Fotos/modificada/IMG_7233_modificada.jpeg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1CDyQFIUCuR"
      },
      "outputs": [],
      "source": [
        "gui.search_image(\"/content/drive/MyDrive/DevMaster/Fotos/produtos/Anel/201-400/IMG_6727.jpeg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abX1lXkeOCyS"
      },
      "outputs": [],
      "source": [
        "gui.search_image(\"/content/drive/MyDrive/DevMaster/Fotos/modificada/094-modificada.png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1dnexC0-5KdjyI4hWCoD2S8iUIYT0yl2g",
      "authorship_tag": "ABX9TyMkailEAnQ+ZkR/v94ROhi6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}